{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nV8z03nut5nN",
        "outputId": "d2274b43-81b0-4eb7-d1eb-37409558c509"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/drive/MyDrive/nlp100/chapter09\n"
          ]
        }
      ],
      "source": [
        "!pwd\n",
        "%cd drive/MyDrive/nlp100/chapter09"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# knock50\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "# FORMAT: ID \\t TITLE \\t URL \\t PUBLISHER \\t CATEGORY \\t STORY \\t HOSTNAME \\t TIMESTAMP\n",
        "df = pd.read_csv(\"newsCorpora.csv\", sep=\"\\t\", header=None, names=[\"ID\", \"TITLE\", \"URL\", \"PUBLISHER\", \"CATEGORY\", \"STORY\", \"HOSTNAME\", \"TIMESTAMP\"])\n",
        "\n",
        "# 該当するpublisherの記事を抽出する\n",
        "publishers = ['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']\n",
        "df = df[df['PUBLISHER'].isin(publishers)]\n",
        "# TITLEとCATEGORYのみ抽出\n",
        "df = df[[\"TITLE\", \"CATEGORY\"]]\n",
        "\n",
        "#データを分割しシャッフルする\n",
        "train, test = train_test_split(df, test_size=0.2, shuffle=True)\n",
        "test, valid = train_test_split(test, test_size=0.5, shuffle=True)\n",
        "\n",
        "#ファイルに保存する\n",
        "train.to_csv(\"train.txt\", sep=\"\\t\", index=False, header=None)\n",
        "valid.to_csv(\"valid.txt\", sep=\"\\t\", index=False, header=None)\n",
        "test.to_csv(\"test.txt\", sep=\"\\t\", index=False, header=None)\n",
        "\n",
        "print(\"train\\n\", train[\"CATEGORY\"].value_counts())\n",
        "print(\"valid\\n\", valid[\"CATEGORY\"].value_counts())\n",
        "print(\"test\\n\", test[\"CATEGORY\"].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKJ5FXO-uAgZ",
        "outputId": "09683428-6251-4266-8d7e-f9ab8c067c69"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train\n",
            " b    4456\n",
            "e    4253\n",
            "t    1227\n",
            "m     736\n",
            "Name: CATEGORY, dtype: int64\n",
            "valid\n",
            " b    591\n",
            "e    496\n",
            "t    153\n",
            "m     94\n",
            "Name: CATEGORY, dtype: int64\n",
            "test\n",
            " b    580\n",
            "e    530\n",
            "t    144\n",
            "m     80\n",
            "Name: CATEGORY, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# knock80\n",
        "from collections import defaultdict\n",
        "import string\n",
        "\n",
        "# 頻度を数える\n",
        "d = defaultdict(int) # 初期値を0にする\n",
        "table = str.maketrans(string.punctuation, ' '*len(string.punctuation)) # 記号をスペースに置換する変換を記述\n",
        "for text in train['TITLE']:\n",
        "  for word in text.translate(table).split(): # スペースに変換する操作を実行し、区切って単語を格納。単語を一つ一つ見ていく\n",
        "    d[word] += 1\n",
        "d = sorted(d.items(), key=lambda x:x[1], reverse=True) # ソートする\n",
        "\n",
        "# ID辞書を作成\n",
        "id_dict = {word : i+1 for i, (word, cnt) in enumerate(d) if cnt > 1} # 出現頻度が高い順にidを振っていく\n",
        "\n",
        "def tokenizer(text, id_dict=id_dict, unk=0):\n",
        "  table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
        "  return [id_dict.get(word, unk) for word in text.translate(table).split()] # .get()を使うことで、keyに値が存在しない場合もエラーが出ず、初期値(0に設定してある)を取得する\n",
        "\n",
        "text = train.iloc[1, train.columns.get_loc('TITLE')]\n",
        "print(text)\n",
        "print(tokenizer(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2Erdcn0uf7_",
        "outputId": "a5191022-9a8a-4626-8a98-c60138d1bc12"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spanish Bonds Rise With Italy's as Month-High Yield Lures Buyers\n",
            "[1159, 351, 207, 21, 2088, 2, 6, 213, 161, 2089, 5200, 3545]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# knock81\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "# RNNの作成\n",
        "# モデルの構築\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.utils.data as data\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size, padding_idx, hidden_size, output_size, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n",
        "        self.rnn = nn.LSTM(emb_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, h0=None):\n",
        "        x = self.emb(x)\n",
        "        x, h = self.rnn(x, h0)\n",
        "        x = x[:, -1, :]\n",
        "        logits = self.fc(x)\n",
        "        return logits\n",
        "\n",
        "# パラメータの設定\n",
        "VOCAB_SIZE = len(set(id_dict.values())) + 2  # 辞書のID数 + unknown + パディングID\n",
        "EMB_SIZE = 300\n",
        "PADDING_IDX = len(set(id_dict.values())) + 1\n",
        "OUTPUT_SIZE = 4\n",
        "HIDDEN_SIZE = 50\n",
        "NUM_LAYERS = 1\n",
        "\n",
        "# モデルの定義\n",
        "model = RNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, HIDDEN_SIZE, OUTPUT_SIZE, NUM_LAYERS)\n",
        "\n",
        "text = train.iloc[1, train.columns.get_loc('TITLE')]\n",
        "x = torch.tensor([tokenizer(text)], dtype=torch.int64)\n",
        "print(x)\n",
        "print(x.size())\n",
        "print(nn.Softmax(dim=-1)(model(x)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezUkgFwexzD2",
        "outputId": "362a7af2-9926-4da0-9f51-99b703ff6b2b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[4228,   56,  196,  192,    1,  235,  367,    1, 2343,  298]])\n",
            "torch.Size([1, 10])\n",
            "tensor([[0.2786, 0.2446, 0.2233, 0.2534]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train['TITLE'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5kZmz2KtUxD",
        "outputId": "35046408-38d3-4b54-d07a-c52ba1f8f168"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "406661    Nasa's 2020 Mars Rover mission revealed: A dev...\n",
            "47892             NYT CEO: We Have to Get Back to Ad Growth\n",
            "11856     Keith Richards' Children's Book Inspired By Hi...\n",
            "299697    Pregnant Kourtney Kardashian flashes her midri...\n",
            "405264    UPDATE 2-Kerry presses India on global trade d...\n",
            "                                ...                        \n",
            "300896    UPDATE 1-Britain's cost agency not ready to ba...\n",
            "316902    White House: no change to US policy on crude o...\n",
            "363818    Russia's Lavrov to talk South Stream pipeline ...\n",
            "401048    COLUMN-Fed to widen Main St/Wall St gap: James...\n",
            "137983    Home > Prince > Prince Re-signs With Warner Br...\n",
            "Name: TITLE, Length: 10672, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# knock82\n",
        "\n",
        "# カテゴリ名を数字に変更\n",
        "category_dict = {'b': 0, 't': 1, 'e':2, 'm':3}\n",
        "Y_train = torch.from_numpy(train['CATEGORY'].map(category_dict).values)\n",
        "Y_valid = torch.from_numpy(valid['CATEGORY'].map(category_dict).values)\n",
        "Y_test = torch.from_numpy(test['CATEGORY'].map(category_dict).values)\n",
        "\n",
        "class NewsDataset(data.Dataset):\n",
        "    def __init__(self, X, y, phase='train'):\n",
        "        self.X = X['TITLE']\n",
        "        self.y = y\n",
        "        self.phase = phase\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # idxに対応するデータとラベルを取得\n",
        "        inputs = torch.tensor(tokenizer(self.X.values[idx]))\n",
        "        return inputs, self.y[idx]\n",
        "\n",
        "train_dataset = NewsDataset(train, Y_train, phase='train')\n",
        "valid_dataset = NewsDataset(valid, Y_valid, phase='val')\n",
        "test_dataset = NewsDataset(test, Y_test, phase='val')\n",
        "idx = 0\n",
        "print(train_dataset.__getitem__(idx)[0].size())\n",
        "print(train_dataset.__getitem__(idx)[1])\n",
        "print(valid_dataset.__getitem__(idx)[0].size())\n",
        "print(valid_dataset.__getitem__(idx)[1])\n",
        "print(test_dataset.__getitem__(idx)[0].size())\n",
        "print(test_dataset.__getitem__(idx)[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggXZ1LoEq-WG",
        "outputId": "48e9aab2-9603-4ccc-d825-f1d4f18152a8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([13])\n",
            "tensor(1)\n",
            "torch.Size([10])\n",
            "tensor(2)\n",
            "torch.Size([11])\n",
            "tensor(2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# knock81\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self, vocab_size, emb_size, padding_idx, output_size, hidden_size):\n",
        "    super().__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n",
        "    self.rnn = nn.RNN(emb_size, hidden_size, nonlinearity='tanh', batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.batch_size = x.size()[0]\n",
        "    hidden = self.init_hidden(x.device)  # h0のゼロベクトルを作成\n",
        "    emb = self.emb(x)\n",
        "    # emb.size() = (batch_size, seq_len, emb_size)\n",
        "    out, hidden = self.rnn(emb, hidden)\n",
        "    # out.size() = (batch_size, seq_len, hidden_size)\n",
        "    out = self.fc(out[:, -1, :])\n",
        "    # out.size() = (batch_size, output_size)\n",
        "    return out\n",
        "\n",
        "  def init_hidden(self, device):\n",
        "    hidden = torch.zeros(1, self.batch_size, self.hidden_size, device=device)\n",
        "    return hidden\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CreateDataset(Dataset):\n",
        "  def __init__(self, X, y, tokenizer):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def __len__(self):  # len(Dataset)で返す値を指定\n",
        "    return len(self.y)\n",
        "\n",
        "  def __getitem__(self, index):  # Dataset[index]で返す値を指定\n",
        "    text = self.X.values[index]\n",
        "    inputs = self.tokenizer(text)\n",
        "\n",
        "    return {\n",
        "      'inputs': torch.tensor(inputs, dtype=torch.int64),\n",
        "      'labels': torch.tensor(self.y[index], dtype=torch.int64)\n",
        "    }\n",
        "\n",
        "# ラベルベクトルの作成\n",
        "category_dict = {'b': 0, 't': 1, 'e':2, 'm':3}\n",
        "y_train = train['CATEGORY'].map(lambda x: category_dict[x]).values\n",
        "y_valid = valid['CATEGORY'].map(lambda x: category_dict[x]).values\n",
        "y_test = test['CATEGORY'].map(lambda x: category_dict[x]).values\n",
        "\n",
        "# Datasetの作成\n",
        "dataset_train = CreateDataset(train['TITLE'], y_train, tokenizer)\n",
        "dataset_valid = CreateDataset(valid['TITLE'], y_valid, tokenizer)\n",
        "dataset_test = CreateDataset(test['TITLE'], y_test, tokenizer)\n",
        "\n",
        "# パラメータの設定\n",
        "VOCAB_SIZE = len(set(id_dict.values())) + 1  # 辞書のID数 + パディングID\n",
        "EMB_SIZE = 300\n",
        "PADDING_IDX = len(set(id_dict.values()))\n",
        "OUTPUT_SIZE = 4\n",
        "HIDDEN_SIZE = 50\n",
        "\n",
        "# モデルの定義\n",
        "model = RNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, OUTPUT_SIZE, HIDDEN_SIZE)\n",
        "\n",
        "# 先頭10件の予測値取得\n",
        "for i in range(10):\n",
        "  X = dataset_train[i]['inputs']\n",
        "  print(torch.softmax(model(X.unsqueeze(0)), dim=-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxmNMJqr9kGI",
        "outputId": "6ff24c5b-8f2a-401e-d3b0-9d08ff62ece1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1756, 0.2869, 0.3298, 0.2077]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.2283, 0.1276, 0.2835, 0.3606]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.1995, 0.3004, 0.2377, 0.2624]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.2422, 0.1170, 0.3871, 0.2537]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.3368, 0.3066, 0.1375, 0.2191]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.2198, 0.2599, 0.1523, 0.3680]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.3548, 0.1676, 0.1060, 0.3715]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.3118, 0.2248, 0.2427, 0.2206]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.2766, 0.2314, 0.2090, 0.2830]], grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[0.3157, 0.2440, 0.2217, 0.2186]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# knock82\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "from torch import optim\n",
        "\n",
        "def calculate_loss_and_accuracy(model, dataset, device=None, criterion=None):\n",
        "  \"\"\"損失・正解率を計算\"\"\"\n",
        "  dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "  loss = 0.0\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data in dataloader:\n",
        "      # デバイスの指定\n",
        "      inputs = data['inputs'].to(device)\n",
        "      labels = data['labels'].to(device)\n",
        "\n",
        "      # 順伝播\n",
        "      outputs = model(inputs)\n",
        "\n",
        "      # 損失計算\n",
        "      if criterion != None:\n",
        "        loss += criterion(outputs, labels).item()\n",
        "\n",
        "      # 正解率計算\n",
        "      pred = torch.argmax(outputs, dim=-1)\n",
        "      total += len(inputs)\n",
        "      correct += (pred == labels).sum().item()\n",
        "\n",
        "  return loss / len(dataset), correct / total\n",
        "\n",
        "\n",
        "def train_model(dataset_train, dataset_valid, batch_size, model, criterion, optimizer, num_epochs, collate_fn=None, device=None):\n",
        "  # モデルの学習を実行し、損失・正解率のログを返す\n",
        "  # デバイスの指定\n",
        "  model.to(device)\n",
        "\n",
        "  # dataloaderの作成\n",
        "  dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "  dataloader_valid = DataLoader(dataset_valid, batch_size=1, shuffle=False)\n",
        "\n",
        "  # スケジューラの設定\n",
        "  scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs, eta_min=1e-5, last_epoch=-1)\n",
        "\n",
        "  # 学習\n",
        "  log_train = []\n",
        "  log_valid = []\n",
        "  for epoch in range(num_epochs):\n",
        "    # 開始時刻の記録\n",
        "    s_time = time.time()\n",
        "\n",
        "    # 訓練モードに設定\n",
        "    model.train()\n",
        "    for data in dataloader_train:\n",
        "      # 勾配をゼロで初期化\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # 順伝播 + 誤差逆伝播 + 重み更新\n",
        "      inputs = data['inputs'].to(device)\n",
        "      labels = data['labels'].to(device)\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    # 評価モードに設定\n",
        "    model.eval()\n",
        "\n",
        "    # 損失と正解率の算出\n",
        "    loss_train, acc_train = calculate_loss_and_accuracy(model, dataset_train, device, criterion=criterion)\n",
        "    loss_valid, acc_valid = calculate_loss_and_accuracy(model, dataset_valid, device, criterion=criterion)\n",
        "    log_train.append([loss_train, acc_train])\n",
        "    log_valid.append([loss_valid, acc_valid])\n",
        "\n",
        "\n",
        "    torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, f'checkpoint{epoch + 1}.pt')\n",
        "\n",
        "    e_time = time.time()\n",
        "    # ログを出力\n",
        "    print(f'epoch: {epoch + 1}, loss_train: {loss_train:.4f}, accuracy_train: {acc_train:.4f}, loss_valid: {loss_valid:.4f}, accuracy_valid: {acc_valid:.4f}, {(e_time - s_time):.4f}sec')\n",
        "\n",
        "    # 検証データの損失が3エポック連続で低下しなかった場合は学習終了\n",
        "    if epoch > 2 and log_valid[epoch - 3][0] <= log_valid[epoch - 2][0] <= log_valid[epoch - 1][0] <= log_valid[epoch][0]:\n",
        "      break\n",
        "\n",
        "    # スケジューラを1ステップ進める\n",
        "    scheduler.step()\n",
        "\n",
        "  return {'train': log_train, 'valid': log_valid}\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def visualize_logs(log):\n",
        "  fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
        "  ax[0].plot(np.array(log['train']).T[0], label='train')\n",
        "  ax[0].plot(np.array(log['valid']).T[0], label='valid')\n",
        "  ax[0].set_xlabel('epoch')\n",
        "  ax[0].set_ylabel('loss')\n",
        "  ax[0].legend()\n",
        "  ax[1].plot(np.array(log['train']).T[1], label='train')\n",
        "  ax[1].plot(np.array(log['valid']).T[1], label='valid')\n",
        "  ax[1].set_xlabel('epoch')\n",
        "  ax[1].set_ylabel('accuracy')\n",
        "  ax[1].legend()\n",
        "  plt.show()\n",
        "\n",
        "# パラメータの設定\n",
        "VOCAB_SIZE = len(set(id_dict.values())) + 1\n",
        "EMB_SIZE = 300\n",
        "PADDING_IDX = len(set(id_dict.values()))\n",
        "OUTPUT_SIZE = 4\n",
        "HIDDEN_SIZE = 50\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 1\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "# モデルの定義\n",
        "model = RNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, OUTPUT_SIZE, HIDDEN_SIZE)\n",
        "\n",
        "# 損失関数の定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# オプティマイザの定義\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# モデルの学習\n",
        "log = train_model(dataset_train, dataset_valid, BATCH_SIZE, model, criterion, optimizer, NUM_EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qz2Qqc5aufBk",
        "outputId": "e2fc014d-e5bf-4515-a95f-0fb4d7bd7ca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1, loss_train: 1.1119, accuracy_train: 0.5183, loss_valid: 1.1287, accuracy_valid: 0.4948, 42.0904sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JFmxUnYyuw0W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}