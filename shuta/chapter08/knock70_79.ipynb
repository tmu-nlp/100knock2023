{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 第8章: ニューラルネット\n",
        "\n",
        "第6章で取り組んだニュース記事のカテゴリ分類を題材として，ニューラルネットワークでカテゴリ分類モデルを実装する．なお，この章ではPyTorch, TensorFlow, Chainerなどの機械学習プラットフォームを活用せよ．"
      ],
      "metadata": {
        "id": "2yngkOZumoER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#colabを利用する上での前準備\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-0AqQjdfnNC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd '/content/drive/MyDrive'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09Y_OvUWm_Lh",
        "outputId": "7210e111-a5ea-41bf-8c39-e8f8cc1a70a4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8qcpBddnWxy",
        "outputId": "0708064e-87ac-47c3-956c-2510ee64f50e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 70. 単語ベクトルの和による特徴量\n",
        "\n",
        "問題50で構築した学習データ，検証データ，評価データを行列・ベクトルに変換したい．例えば，学習データについて，すべての事例xi\n",
        "の特徴ベクトルxi\n",
        "を並べた行列X\n",
        "と，正解ラベルを並べた行列（ベクトル）Y\n",
        "を作成したい．\n",
        "\n",
        "ここで，n\n",
        "は学習データの事例数であり，xi∈ℝd\n",
        "とyi∈ℕ\n",
        "はそれぞれ，i∈{1,…,n}\n",
        "番目の事例の特徴量ベクトルと正解ラベルを表す． なお，今回は「ビジネス」「科学技術」「エンターテイメント」「健康」の4カテゴリ分類である．ℕ<4\n",
        "で4\n",
        "未満の自然数（0\n",
        "を含む）を表すことにすれば，任意の事例の正解ラベルyi\n",
        "はyi∈ℕ<4\n",
        "で表現できる． 以降では，ラベルの種類数をL\n",
        "で表す（今回の分類タスクではL=4\n",
        "である）．\n",
        "\n",
        "i\n",
        "番目の事例の特徴ベクトルxi\n",
        "は，次式で求める．\n",
        "\n",
        "ここで，i\n",
        "番目の事例はTi\n",
        "個の（記事見出しの）単語列(wi,1,wi,2,…,wi,Ti)\n",
        "から構成され，emb(w)∈ℝd\n",
        "は単語w\n",
        "に対応する単語ベクトル（次元数はd\n",
        "）である．すなわち，i\n",
        "番目の事例の記事見出しを，その見出しに含まれる単語のベクトルの平均で表現したものがxi\n",
        "である．今回は単語ベクトルとして，問題60でダウンロードしたものを用いればよい．300\n",
        "次元の単語ベクトルを用いたので，d=300\n",
        "である．\n",
        "\n",
        "i\n",
        "番目の事例のラベルyi\n",
        "は，次のように定義する．\n",
        "\n",
        "なお，カテゴリ名とラベルの番号が一対一で対応付いていれば，上式の通りの対応付けでなくてもよい．\n",
        "\n",
        "以上の仕様に基づき，以下の行列・ベクトルを作成し，ファイルに保存せよ．\n",
        "\n",
        "学習データの特徴量行列: Xtrain∈ℝNt×d\n",
        "学習データのラベルベクトル: Ytrain∈ℕNt\n",
        "検証データの特徴量行列: Xvalid∈ℝNv×d\n",
        "検証データのラベルベクトル: Yvalid∈ℕNv\n",
        "評価データの特徴量行列: Xtest∈ℝNe×d\n",
        "評価データのラベルベクトル: Ytest∈ℕNe\n",
        "なお，Nt,Nv,Ne\n",
        "はそれぞれ，学習データの事例数，検証データの事例数，評価データの事例数である．\n",
        "\n"
      ],
      "metadata": {
        "id": "6wYM4EE0mrzm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Yn8Av2BSgZeR"
      },
      "outputs": [],
      "source": [
        "#50と同様\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#データ読み込み #エラーが出るため、タブ区切り、ラベル名を指定する\n",
        "df = pd.read_csv('./NewsAggregatorDataset/newsCorpora.csv', sep=\"\\t\", names=[\"id\", \"title\", \"url\", \"publisher\", \"category\", \"story\", \"hostname\", \"timestamp\"])\n",
        "\n",
        "#情報源（publisher）が”Reuters”, “Huffington Post”, “Businessweek”, “Contactmusic.com”, “Daily Mail”の事例（記事）のみを抽出する\n",
        "df = df[df['publisher'].isin([\"Reuters\", \"Huffington Post\", \"Businessweek\", \"COntactmusic.com\", \"Daily Mail\"])]\n",
        "\n",
        "#抽出された事例をランダムに並び替える\n",
        "df = df.sample(frac = 1, random_state = 42, ignore_index=True)\n",
        "\n",
        "#抽出された事例の80%を学習データ，残りの10%ずつを検証データと評価データに分割し，それぞれtrain.txt，valid.txt，test.txtというファイル名で保存する\n",
        "train, test = train_test_split(df, test_size=0.2, random_state = 42)\n",
        "test, valid = train_test_split(test, test_size=0.5, random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#60と同様\n",
        "\n",
        "import gensim\n",
        "\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('./shogi/GoogleNews-vectors-negative300.bin.gz', binary= True)"
      ],
      "metadata": {
        "id": "HnijXr3znmby"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "train['title']"
      ],
      "metadata": {
        "id": "12-HALFaoPWa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lis_train_vecs =[] #1文内の単語ベクトルのリスト\n",
        "tensor_vecs = [] #平均単語ベクトルのリスト\n",
        "for title in train['title']:\n",
        "  lis_train_words = title.split()\n",
        "  for word in lis_train_words:\n",
        "    try:\n",
        "      vec = model[word]\n",
        "      lis_train_vecs.append(vec)\n",
        "\n",
        "    except: #未知語の場合\n",
        "      pass\n",
        "\n",
        "  tensor_vec = torch.tensor(sum(lis_train_vecs) / len(lis_train_vecs))\n",
        "  tensor_vecs.append(tensor_vec)\n",
        "  lis_train_vecs =[]\n"
      ],
      "metadata": {
        "id": "TCDhV0VSovwz"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lis_train_vecs[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrIJ22rUxntn",
        "outputId": "fcfa04c6-2b5d-41fb-ebe0-8fea001d5da5"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 7.91015625e-02,  9.66796875e-02, -1.08642578e-02,  2.56347656e-02,\n",
              "       -3.24707031e-02,  1.09863281e-01, -1.12792969e-01, -4.44335938e-02,\n",
              "        1.07421875e-02, -8.30078125e-03,  2.07519531e-02, -2.78320312e-02,\n",
              "       -7.95898438e-02, -4.93164062e-02, -1.49414062e-01,  5.71289062e-02,\n",
              "        1.57226562e-01, -1.85546875e-02, -1.24023438e-01, -3.36914062e-02,\n",
              "       -3.50952148e-03,  3.68652344e-02, -2.03857422e-02, -1.08398438e-01,\n",
              "        7.47070312e-02, -8.74023438e-02, -1.53320312e-01,  1.07910156e-01,\n",
              "       -2.53906250e-02, -1.08886719e-01,  7.51953125e-02,  1.17675781e-01,\n",
              "        4.88281250e-02,  3.33786011e-04,  1.22558594e-01, -5.37109375e-02,\n",
              "        9.27734375e-02, -3.39355469e-02, -1.06445312e-01,  9.47265625e-02,\n",
              "        4.05273438e-02,  6.25000000e-02,  4.05273438e-02, -6.49414062e-02,\n",
              "       -3.83300781e-02, -3.14941406e-02, -4.73632812e-02, -1.86767578e-02,\n",
              "        1.42211914e-02,  2.90527344e-02, -8.88671875e-02,  1.64062500e-01,\n",
              "        3.56445312e-02, -5.88378906e-02, -8.49609375e-02,  8.78906250e-02,\n",
              "       -3.17382812e-02,  5.61523438e-02, -8.93554688e-02, -6.49414062e-02,\n",
              "        8.64257812e-02,  5.41992188e-02, -6.49414062e-02, -3.61328125e-02,\n",
              "       -3.08837891e-02, -1.80816650e-03,  9.81445312e-02,  3.97949219e-02,\n",
              "        5.68847656e-02,  3.75976562e-02,  6.68945312e-02,  1.63574219e-02,\n",
              "        7.87353516e-03, -1.44653320e-02, -6.64062500e-02, -8.78906250e-02,\n",
              "       -7.12890625e-02,  2.61718750e-01, -5.49316406e-03,  8.64257812e-02,\n",
              "        6.68945312e-02,  1.43554688e-01, -2.13623047e-02,  2.49023438e-02,\n",
              "       -1.04492188e-01, -1.26953125e-01, -1.22558594e-01,  8.59375000e-02,\n",
              "        2.58789062e-02, -1.16210938e-01, -2.91748047e-02,  1.27929688e-01,\n",
              "       -1.03027344e-01, -1.41601562e-01,  4.78515625e-02, -2.69775391e-02,\n",
              "        6.78710938e-02,  3.00292969e-02, -1.36718750e-01, -6.83593750e-02,\n",
              "       -3.63769531e-02, -1.45507812e-01,  6.49414062e-02,  1.12304688e-02,\n",
              "        1.68457031e-02,  5.56640625e-02,  7.47680664e-03, -2.90527344e-02,\n",
              "        2.45117188e-01, -2.28271484e-02,  1.15234375e-01,  6.17675781e-02,\n",
              "       -9.91210938e-02,  9.22851562e-02,  1.20117188e-01, -8.98437500e-02,\n",
              "       -1.09375000e-01, -8.74023438e-02,  1.07910156e-01,  3.32031250e-02,\n",
              "       -3.51562500e-01,  1.58203125e-01, -4.85839844e-02, -3.01513672e-02,\n",
              "       -3.41796875e-02, -1.03759766e-03,  7.17773438e-02, -1.15722656e-01,\n",
              "        1.09375000e-01,  3.02124023e-03,  2.39257812e-02, -1.81640625e-01,\n",
              "       -1.23046875e-01, -1.97753906e-02, -9.71679688e-02, -1.61132812e-01,\n",
              "       -1.81640625e-01,  9.96093750e-02, -9.81445312e-02,  1.46484375e-02,\n",
              "        9.08203125e-02, -3.66210938e-02,  3.44238281e-02, -4.63867188e-02,\n",
              "        7.76367188e-02,  5.51757812e-02,  7.37304688e-02, -6.73828125e-02,\n",
              "       -1.37695312e-01, -2.18505859e-02,  1.13281250e-01,  3.85742188e-02,\n",
              "       -1.31835938e-01,  1.18164062e-01,  5.34667969e-02,  1.87988281e-02,\n",
              "       -1.42578125e-01,  1.23291016e-02,  2.33398438e-01, -8.93554688e-02,\n",
              "       -1.25732422e-02,  1.73828125e-01,  1.52343750e-01,  7.51953125e-02,\n",
              "       -4.49218750e-02,  2.03857422e-02, -4.11987305e-03,  4.41894531e-02,\n",
              "       -8.78906250e-02, -6.54296875e-02, -1.13769531e-01,  1.18652344e-01,\n",
              "       -1.40380859e-02, -1.87500000e-01,  7.27539062e-02,  1.67236328e-02,\n",
              "        1.86767578e-02,  4.24804688e-02, -8.78906250e-02, -3.44238281e-02,\n",
              "        1.25976562e-01, -1.99218750e-01,  6.05468750e-02,  8.72802734e-03,\n",
              "        5.56640625e-02, -9.32617188e-02,  2.60925293e-03,  4.93164062e-02,\n",
              "        1.10839844e-01, -9.76562500e-03,  2.30712891e-02,  1.50390625e-01,\n",
              "       -1.35498047e-02, -8.05664062e-02,  9.22851562e-02, -3.01513672e-02,\n",
              "        1.25732422e-02,  7.47070312e-02, -1.81640625e-01,  7.14111328e-03,\n",
              "       -4.56542969e-02, -8.23974609e-04,  5.49316406e-02, -1.45874023e-02,\n",
              "        7.08007812e-02, -2.00195312e-01, -1.51824951e-03,  6.59179688e-02,\n",
              "       -4.12597656e-02, -1.51367188e-01, -7.91015625e-02, -7.47070312e-02,\n",
              "       -8.93554688e-02,  3.83300781e-02, -3.54003906e-02, -5.61523438e-02,\n",
              "       -1.59179688e-01,  2.64892578e-02, -1.09375000e-01, -7.32421875e-02,\n",
              "        3.49121094e-02, -1.57470703e-02,  8.00781250e-02, -3.34472656e-02,\n",
              "        2.08984375e-01, -5.34057617e-03,  1.25976562e-01,  1.01562500e-01,\n",
              "        1.51367188e-01, -7.27539062e-02,  8.05664062e-02, -1.34765625e-01,\n",
              "        2.74658203e-02,  7.81250000e-02,  2.19726562e-02,  1.87988281e-02,\n",
              "       -5.56640625e-02, -5.00488281e-02, -7.17773438e-02, -6.25000000e-02,\n",
              "       -9.91210938e-02, -8.88671875e-02,  2.09960938e-02,  4.49218750e-02,\n",
              "       -5.92041016e-03,  1.66015625e-01,  1.29882812e-01, -8.93554688e-02,\n",
              "       -3.49121094e-02, -1.85546875e-02,  1.27929688e-01, -5.54199219e-02,\n",
              "        4.76074219e-02,  4.73632812e-02,  9.57031250e-02, -1.66015625e-02,\n",
              "        1.31835938e-01,  6.88476562e-02, -2.03125000e-01, -5.41992188e-02,\n",
              "       -6.83593750e-02,  6.07910156e-02, -1.54296875e-01,  9.81445312e-02,\n",
              "        1.28906250e-01,  6.29882812e-02,  8.78906250e-02,  2.50000000e-01,\n",
              "        2.27050781e-02,  1.09375000e-01,  1.87500000e-01, -1.25976562e-01,\n",
              "        1.10473633e-02,  8.39843750e-02, -1.18652344e-01,  5.71289062e-02,\n",
              "        3.46679688e-02,  2.09960938e-02,  5.51757812e-02, -3.17382812e-02,\n",
              "        6.22558594e-02, -1.12304688e-01,  5.20019531e-02,  1.50390625e-01,\n",
              "       -9.94873047e-03, -3.71093750e-02, -1.22558594e-01, -3.44238281e-02,\n",
              "        3.63769531e-02,  3.93066406e-02,  8.04901123e-04, -9.15527344e-03,\n",
              "       -3.49121094e-02, -8.20312500e-02, -4.32128906e-02, -5.00488281e-02,\n",
              "        4.08935547e-03,  5.27343750e-02,  7.12890625e-02, -5.00488281e-02],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ターゲットのテンソル化\n",
        "category_dict = {'b': 0, 't': 1, 'e':2, 'm':3}\n",
        "Y_train = torch.from_numpy(train['category'].map(category_dict).values)\n",
        "Y_valid = torch.from_numpy(valid['category'].map(category_dict).values)\n",
        "Y_test = torch.from_numpy(test['category'].map(category_dict).values)\n",
        "# 保存\n",
        "torch.save(X_train, './NewsAggregatorDataset/X_train.pt')\n",
        "torch.save(X_valid, './NewsAggregatorDataset/X_valid.pt')\n",
        "torch.save(X_test, './NewsAggregatorDataset/X_test.pt')\n",
        "torch.save(Y_train, './NewsAggregatorDataset/y_train.pt')\n",
        "torch.save(Y_valid, './NewsAggregatorDataset/y_valid.pt')\n",
        "torch.save(Y_test, './NewsAggregatorDataset/y_test.pt')"
      ],
      "metadata": {
        "id": "9gg59WAV-wjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.stack(tensor_vecs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQ-9EGOCpOtC",
        "outputId": "159b9a41-1be8-44b0-a3c5-411edf38483d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0115,  0.0006,  0.0168,  ..., -0.0010,  0.0009,  0.0061],\n",
              "       dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wMKzT6Raw5K0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 72. 損失と勾配の計算\n",
        "\n",
        "学習データの事例x1\n",
        "と事例集合x1,x2,x3,x4\n",
        "に対して，クロスエントロピー損失と，行列W\n",
        "に対する勾配を計算せよ．なお，ある事例xi\n",
        "に対して損失は次式で計算される．\n",
        "\n",
        "li=−log[事例xiがyiに分類される確率]\n",
        "ただし，事例集合に対するクロスエントロピー損失は，その集合に含まれる各事例の損失の平均とする．\n",
        "\n"
      ],
      "metadata": {
        "id": "4HkNSJUa_xlA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dDgYeuHx_62v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 73. 確率的勾配降下法による学習"
      ],
      "metadata": {
        "id": "7CUOettA_7LR"
      }
    }
  ]
}