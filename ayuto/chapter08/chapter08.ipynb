{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# データの読込\n",
    "df = pd.read_csv('./newsCorpora_re.csv', header=None, sep='\\t', names=['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n",
    "\n",
    "# データの抽出\n",
    "df = df.loc[df['PUBLISHER'].isin(['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']), ['TITLE', 'CATEGORY']]\n",
    "\n",
    "# データの分割\n",
    "train, valid_test = train_test_split(df, test_size=0.2, shuffle=True, random_state=123, stratify=df['CATEGORY'])\n",
    "valid, test = train_test_split(valid_test, test_size=0.5, shuffle=True, random_state=123, stratify=valid_test['CATEGORY'])\n",
    "\n",
    "# 出力\n",
    "print(train['CATEGORY'].value_counts())\n",
    "print(valid['CATEGORY'].value_counts())\n",
    "print(test['CATEGORY'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# 学習済み単語ベクトルのダウンロード\n",
    "url = \"https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM\"\n",
    "output = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "gdown.download(url, output, quiet=True)\n",
    " \n",
    "# ダウンロードファイルのロード\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import torch\n",
    "\n",
    "def transform_w2v(text):\n",
    "  table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "  words = text.translate(table).split()  # 記号をスペースに置換後、スペースで分割してリスト化\n",
    "  vec = [model[word] for word in words if word in model]  # 1語ずつベクトル化\n",
    "\n",
    "  return torch.tensor(sum(vec) / len(vec))  # 平均ベクトルをTensor型に変換して出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴ベクトルの作成\n",
    "X_train = torch.stack([transform_w2v(text) for text in train['TITLE']])\n",
    "X_valid = torch.stack([transform_w2v(text) for text in valid['TITLE']])\n",
    "X_test = torch.stack([transform_w2v(text) for text in test['TITLE']])\n",
    "\n",
    "print(X_train.size())\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ラベルベクトルの作成\n",
    "category_dict = {'b': 0, 't': 1, 'e':2, 'm':3}\n",
    "y_train = torch.LongTensor(train['CATEGORY'].map(lambda x: category_dict[x]).values)\n",
    "y_valid = torch.LongTensor(valid['CATEGORY'].map(lambda x: category_dict[x]).values)\n",
    "y_test = torch.LongTensor(test['CATEGORY'].map(lambda x: category_dict[x]).values)\n",
    "\n",
    "print(y_train.size())\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存\n",
    "torch.save(X_train, 'X_train.pt')\n",
    "torch.save(X_valid, 'X_valid.pt')\n",
    "torch.save(X_test, 'X_test.pt')\n",
    "torch.save(y_train, 'y_train.pt')\n",
    "torch.save(y_valid, 'y_valid.pt')\n",
    "torch.save(y_test, 'y_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class SLPNet(nn.Module):\n",
    "  def __init__(self, input_size, output_size):\n",
    "    super().__init__()\n",
    "    self.fc = nn.Linear(input_size, output_size, bias=False)  # Linear(入力次元数, 出力次元数)\n",
    "    nn.init.normal_(self.fc.weight, 0.0, 1.0)  # 正規乱数で重みを初期化\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.fc(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "l_1 = criterion(model.forward(X_train[:1]), y_train[:1])  # 入力ベクトルはsoftmax前の値\n",
    "model.zero_grad()  # 勾配をゼロで初期化\n",
    "l_1.backward()  # 勾配を計算\n",
    "print(f'損失: {l_1:.4f}')\n",
    "print(f'勾配:\\n{model.fc.weight.grad}')\n",
    "l = criterion(model.forward(X_train[:4]), y_train[:4])\n",
    "model.zero_grad()\n",
    "l.backward()\n",
    "print(f'損失: {l:.4f}')\n",
    "print(f'勾配:\\n{model.fc.weight.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "  def __init__(self, X, y):  # datasetの構成要素を指定\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "\n",
    "  def __len__(self):  # len(dataset)で返す値を指定\n",
    "    return len(self.y)\n",
    "\n",
    "  def __getitem__(self, idx):  # dataset[idx]で返す値を指定\n",
    "    return [self.X[idx], self.y[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Datasetの作成\n",
    "Dataset_train = NewsDataset(X_train, y_train)\n",
    "dataset_valid = NewsDataset(X_valid, y_valid)\n",
    "dataset_test = NewsDataset(X_test, y_test)\n",
    "\n",
    "# Dataloaderの作成\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=1, shuffle=True)\n",
    "dataloader_valid = DataLoader(dataset_valid, batch_size=len(dataset_valid), shuffle=False)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=len(dataset_test), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CreateDataset(Dataset):\n",
    "  def __init__(self, X, y):  # datasetの構成要素を指定\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "\n",
    "  def __len__(self):  # len(dataset)で返す値を指定\n",
    "    return len(self.y)\n",
    "\n",
    "  def __getitem__(self, idx):  # dataset[idx]で返す値を指定\n",
    "    if isinstance(idx, torch.Tensor):\n",
    "      idx = idx.tolist()\n",
    "    return [self.X[idx], self.y[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CreateDataset(Dataset):\n",
    "  def __init__(self, X, y):  # datasetの構成要素を指定\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "\n",
    "  def __len__(self):  # len(dataset)で返す値を指定\n",
    "    return len(self.y)\n",
    "\n",
    "  def __getitem__(self, idx):  # dataset[idx]で返す値を指定\n",
    "    if isinstance(idx, torch.Tensor):\n",
    "      idx = idx.tolist()\n",
    "    return [self.X[idx], self.y[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの定義\n",
    "model = SLPNet(300, 4)\n",
    "\n",
    "# 損失関数の定義\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# オプティマイザの定義\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
    "\n",
    "# 学習\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "  # 訓練モードに設定\n",
    "  model.train()\n",
    "  loss_train = 0.0\n",
    "  for i, (inputs, labels) in enumerate(dataloader_train):\n",
    "    # 勾配をゼロで初期化\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 順伝播 + 誤差逆伝播 + 重み更新\n",
    "    outputs = model.forward(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 損失を記録\n",
    "    loss_train += loss.item()\n",
    " \n",
    "  # バッチ単位の平均損失計算\n",
    "  loss_train = loss_train / i\n",
    "\n",
    "  # 検証データの損失計算\n",
    "  model.eval() \n",
    "  with torch.no_grad():\n",
    "    inputs, labels = next(iter(dataloader_valid))\n",
    "    outputs = model.forward(inputs)\n",
    "    loss_valid = criterion(outputs, labels)\n",
    "\n",
    "  # ログを出力\n",
    "  print(f'epoch: {epoch + 1}, loss_train: {loss_train:.4f}, loss_valid: {loss_valid:.4f}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, X, y):\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    outputs = model(X)\n",
    "    pred = torch.argmax(outputs, dim=-1)\n",
    "\n",
    "  return (pred == y).sum().item() / len(y)\n",
    "# 正解率の確認\n",
    "acc_train = calculate_accuracy(model, X_train, y_train)\n",
    "acc_test = calculate_accuracy(model, X_test, y_test)\n",
    "print(f'正解率（学習データ）：{acc_train:.3f}')\n",
    "print(f'正解率（評価データ）：{acc_test:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_and_accuracy(model, criterion, loader):\n",
    "  model.eval()\n",
    "  loss = 0.0\n",
    "  total = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for inputs, labels in loader:\n",
    "      outputs = model(inputs)\n",
    "      loss += criterion(outputs, labels).item()\n",
    "      pred = torch.argmax(outputs, dim=-1)\n",
    "      total += len(inputs)\n",
    "      correct += (pred == labels).sum().item()\n",
    "      \n",
    "  return loss / len(loader), correct / total\n",
    "# モデルの定義\n",
    "model = SLPNet(300, 4)\n",
    "\n",
    "# 損失関数の定義\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# オプティマイザの定義\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
    "\n",
    "# 学習\n",
    "num_epochs = 30\n",
    "log_train = []\n",
    "log_valid = []\n",
    "for epoch in range(num_epochs):\n",
    "  # 訓練モードに設定\n",
    "  model.train()\n",
    "  for inputs, labels in dataloader_train:\n",
    "    # 勾配をゼロで初期化\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 順伝播 + 誤差逆伝播 + 重み更新\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    " \n",
    "  # 損失と正解率の算出\n",
    "  loss_train, acc_train = calculate_loss_and_accuracy(model, criterion, dataloader_train)\n",
    "  loss_valid, acc_valid = calculate_loss_and_accuracy(model, criterion, dataloader_valid)\n",
    "  log_train.append([loss_train, acc_train])\n",
    "  log_valid.append([loss_valid, acc_valid])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# 視覚化\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax[0].plot(np.array(log_train).T[0], label='train')\n",
    "ax[0].plot(np.array(log_valid).T[0], label='valid')\n",
    "ax[0].set_xlabel('epoch')\n",
    "ax[0].set_ylabel('loss')\n",
    "ax[0].legend()\n",
    "ax[1].plot(np.array(log_train).T[1], label='train')\n",
    "ax[1].plot(np.array(log_valid).T[1], label='valid')\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[1].set_ylabel('accuracy')\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの定義\n",
    "model = SLPNet(300, 4)\n",
    "\n",
    "# 損失関数の定義\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# オプティマイザの定義\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
    "\n",
    "# 学習\n",
    "num_epochs = 10\n",
    "log_train = []\n",
    "log_valid = []\n",
    "for epoch in range(num_epochs):\n",
    "  # 訓練モードに設定\n",
    "  model.train()\n",
    "  for inputs, labels in dataloader_train:\n",
    "    # 勾配をゼロで初期化\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 順伝播 + 誤差逆伝播 + 重み更新\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    " \n",
    "  # 損失と正解率の算出\n",
    "  loss_train, acc_train = calculate_loss_and_accuracy(model, criterion, dataloader_train)\n",
    "  loss_valid, acc_valid = calculate_loss_and_accuracy(model, criterion, dataloader_valid)\n",
    "  log_train.append([loss_train, acc_train])\n",
    "  log_valid.append([loss_valid, acc_valid])\n",
    "\n",
    "  # チェックポイントの保存\n",
    "  torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, f'checkpoint{epoch + 1}.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train_model(dataset_train, dataset_valid, batch_size, model, criterion, optimizer, num_epochs):\n",
    "  # dataloaderの作成\n",
    "  dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "  dataloader_valid = DataLoader(dataset_valid, batch_size=len(dataset_valid), shuffle=False)\n",
    "\n",
    "  # 学習\n",
    "  log_train = []\n",
    "  log_valid = []\n",
    "  for epoch in range(num_epochs):\n",
    "    # 開始時刻の記録\n",
    "    s_time = time.time()\n",
    "\n",
    "    # 訓練モードに設定\n",
    "    model.train()\n",
    "    for inputs, labels in dataloader_train:\n",
    "      # 勾配をゼロで初期化\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # 順伝播 + 誤差逆伝播 + 重み更新\n",
    "      outputs = model(inputs)\n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "  \n",
    "    # 損失と正解率の算出\n",
    "    loss_train, acc_train = calculate_loss_and_accuracy(model, criterion, dataloader_train)\n",
    "    loss_valid, acc_valid = calculate_loss_and_accuracy(model, criterion, dataloader_valid)\n",
    "    log_train.append([loss_train, acc_train])\n",
    "    log_valid.append([loss_valid, acc_valid])\n",
    "\n",
    "    # チェックポイントの保存\n",
    "    torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, f'checkpoint{epoch + 1}.pt')\n",
    "\n",
    "    # 終了時刻の記録\n",
    "    e_time = time.time()\n",
    "\n",
    "    # ログを出力\n",
    "    print(f'epoch: {epoch + 1}, loss_train: {loss_train:.4f}, accuracy_train: {acc_train:.4f}, loss_valid: {loss_valid:.4f}, accuracy_valid: {acc_valid:.4f}, {(e_time - s_time):.4f}sec') \n",
    "\n",
    "  return {'train': log_train, 'valid': log_valid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasetの作成\n",
    "dataset_train = CreateDataset(X_train, y_train)\n",
    "dataset_valid = CreateDataset(X_valid, y_valid)\n",
    "\n",
    "# モデルの定義\n",
    "model = SLPNet(300, 4)\n",
    "\n",
    "# 損失関数の定義\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# オプティマイザの定義\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
    "\n",
    "# モデルの学習\n",
    "for batch_size in [2 ** i for i in range(11)]:\n",
    "  print(f'バッチサイズ: {batch_size}')\n",
    "  log = train_model(dataset_train, dataset_valid, batch_size, model, criterion, optimizer, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_and_accuracy(model, criterion, loader, device):\n",
    "  model.eval()\n",
    "  loss = 0.0\n",
    "  total = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for inputs, labels in loader:\n",
    "      inputs = inputs.to(device)\n",
    "      labels = labels.to(device)\n",
    "      outputs = model(inputs)\n",
    "      loss += criterion(outputs, labels).item()\n",
    "      pred = torch.argmax(outputs, dim=-1)\n",
    "      total += len(inputs)\n",
    "      correct += (pred == labels).sum().item()\n",
    "      \n",
    "  return loss / len(loader), correct / total\n",
    "  \n",
    "\n",
    "def train_model(dataset_train, dataset_valid, batch_size, model, criterion, optimizer, num_epochs, device=None):\n",
    "  # GPUに送る\n",
    "  model.to(device)\n",
    "\n",
    "  # dataloaderの作成\n",
    "  dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "  dataloader_valid = DataLoader(dataset_valid, batch_size=len(dataset_valid), shuffle=False)\n",
    "\n",
    "  # 学習\n",
    "  log_train = []\n",
    "  log_valid = []\n",
    "  for epoch in range(num_epochs):\n",
    "    # 開始時刻の記録\n",
    "    s_time = time.time()\n",
    "\n",
    "    # 訓練モードに設定\n",
    "    model.train()\n",
    "    for inputs, labels in dataloader_train:\n",
    "      # 勾配をゼロで初期化\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # 順伝播 + 誤差逆伝播 + 重み更新\n",
    "      inputs = inputs.to(device)\n",
    "      labels = labels.to(device)\n",
    "      outputs = model.forward(inputs)\n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "    \n",
    "    # 損失と正解率の算出\n",
    "    loss_train, acc_train = calculate_loss_and_accuracy(model, criterion, dataloader_train, device)\n",
    "    loss_valid, acc_valid = calculate_loss_and_accuracy(model, criterion, dataloader_valid, device)\n",
    "    log_train.append([loss_train, acc_train])\n",
    "    log_valid.append([loss_valid, acc_valid])\n",
    "\n",
    "    # チェックポイントの保存\n",
    "    torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, f'checkpoint{epoch + 1}.pt')\n",
    "\n",
    "    # 終了時刻の記録\n",
    "    e_time = time.time()\n",
    "\n",
    "    # ログを出力\n",
    "    print(f'epoch: {epoch + 1}, loss_train: {loss_train:.4f}, accuracy_train: {acc_train:.4f}, loss_valid: {loss_valid:.4f}, accuracy_valid: {acc_valid:.4f}, {(e_time - s_time):.4f}sec') \n",
    "\n",
    "  return {'train': log_train, 'valid': log_valid}\n",
    "\n",
    "# datasetの作成\n",
    "dataset_train = CreateDataset(X_train, y_train)\n",
    "dataset_valid = CreateDataset(X_valid, y_valid)\n",
    "\n",
    "# モデルの定義\n",
    "model = SLPNet(300, 4)\n",
    "\n",
    "# 損失関数の定義\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# オプティマイザの定義\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
    "\n",
    "# デバイスの指定\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# モデルの学習\n",
    "for batch_size in [2 ** i for i in range(11)]:\n",
    "  print(f'バッチサイズ: {batch_size}')\n",
    "  log = train_model(dataset_train, dataset_valid, batch_size, model, criterion, optimizer, 1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
