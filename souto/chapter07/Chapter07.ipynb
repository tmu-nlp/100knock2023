{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1"
      ],
      "metadata": {
        "id": "JGYSIbwufONx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM'\n",
        "output = 'GoogleNews-vectors-negative300.bin.gz'\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "metadata": {
        "id": "VaWwdsvcfSab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "model = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "\n",
        "print(model['United_States'])"
      ],
      "metadata": {
        "id": "P_JyGBB1fqB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2"
      ],
      "metadata": {
        "id": "uiL7fCHZfQxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.similarity('United_States', 'U.S.'))"
      ],
      "metadata": {
        "id": "ehAvgkkdi5qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3"
      ],
      "metadata": {
        "id": "RMNCpT6jjEnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.most_similar('United_States', topn=10)"
      ],
      "metadata": {
        "id": "wRwFPkPmj-aQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.most_similar(positive=['United_States'], topn=10)"
      ],
      "metadata": {
        "id": "gWG05390lq6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4"
      ],
      "metadata": {
        "id": "bHmp_wMTkAqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vec = model['Spain'] - model['Madrid'] + model['Athens'] \n",
        "model.most_similar(positive=['Spain', 'Athens'], negative=['Madrid'], topn=10)#結果違う"
      ],
      "metadata": {
        "id": "tx7ylaPIkClV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.most_similar(vec, topn=10)#結果違う"
      ],
      "metadata": {
        "id": "ovffhQaEkw6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5"
      ],
      "metadata": {
        "id": "j0aHNVJ-mKtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://download.tensorflow.org/data/questions-words.txt"
      ],
      "metadata": {
        "id": "SjC38ODfmMHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./questions-words.txt', 'r') as fr:\n",
        "    with open('./ans_of_5.txt', 'w') as fw:\n",
        "        analogy_num=''\n",
        "        for line in fr:\n",
        "          line = line.split()\n",
        "          if line[0] == ':':\n",
        "            analogy_num=line[1]\n",
        "          else:\n",
        "            vec = model[line[1]] - model[line[0]] + model[line[2]]\n",
        "            #add_word, similarity = model.most_similar(vec, topn=1)[0]\n",
        "            add_word, similarity = model.most_similar(positive=[line[1], line[2]], negative=[line[0]], topn=1)[0]\n",
        "            line=' '.join(line)\n",
        "            fw.write(analogy_num + ' ' + line + ' ' + add_word + ' ' + str(similarity) + '\\n')\n"
      ],
      "metadata": {
        "id": "-9OMgfb1rVgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "pBvRdyAygCZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6"
      ],
      "metadata": {
        "id": "nZ37Th0hxfpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sem_cnt = 0\n",
        "sem_cor = 0\n",
        "syn_cnt = 0\n",
        "syn_cor = 0\n",
        "\n",
        "with open('./questions-words-add.txt', 'r') as f:\n",
        "  for line in f:\n",
        "    line = line.split()\n",
        "    if not line[0].startswith('gram'):\n",
        "      sem_cnt += 1\n",
        "      if line[4] == line[5]:\n",
        "        sem_cor += 1\n",
        "    else:\n",
        "      syn_cnt += 1\n",
        "      if line[4] == line[5]:\n",
        "        syn_cor += 1\n",
        "\n",
        "print(f'意味的アナロジー正解率: {sem_cor/sem_cnt:.3f}')\n",
        "print(f'文法的アナロジー正解率: {syn_cor/syn_cnt:.3f}') "
      ],
      "metadata": {
        "id": "MrkRj0vOybIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7"
      ],
      "metadata": {
        "id": "3Q-bX_3uz5n4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://www.gabrilovich.com/resources/data/wordsim353/wordsim353.zip\n",
        "!unzip wordsim353.zip"
      ],
      "metadata": {
        "id": "jmk_k_GUz7Bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "ws353 = []\n",
        "with open('./combined.csv', 'r') as f:\n",
        "  next(f)\n",
        "  for line in f:  # 1行ずつ読込み、単語ベクトルと類似度を計算\n",
        "    line = [s.strip() for s in line.split(',')]\n",
        "    line.append(model.similarity(line[0], line[1]))\n",
        "    ws353.append(line)\n",
        "\n",
        "\n",
        "# スピアマン相関係数の計算\n",
        "human = np.array(ws353).T[2]\n",
        "w2v = np.array(ws353).T[3]\n",
        "correlation, pvalue = spearmanr(human, w2v)\n",
        "\n",
        "print(f'スピアマン相関係数: {correlation:.3f}')"
      ],
      "metadata": {
        "id": "Nham-nR30Fvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8"
      ],
      "metadata": {
        "id": "Rpp2-yRg7MD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 国名の取得\n",
        "countries = set()\n",
        "with open('./ans_of_5.txt') as f:\n",
        "  for line in f:\n",
        "    line = line.split()\n",
        "    if line[0] in ['capital-common-countries', 'capital-world']:\n",
        "      countries.add(line[2])\n",
        "    elif line[0] in ['currency', 'gram6-nationality-adjective']:\n",
        "      countries.add(line[1])\n",
        "countries = list(countries)\n",
        "\n",
        "# 単語ベクトルの取得\n",
        "countries_vec = [model[country] for country in countries]\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# k-meansクラスタリング\n",
        "kmeans = KMeans(n_clusters=5)\n",
        "kmeans.fit(countries_vec)\n",
        "for i in range(5):\n",
        "    cluster = np.where(kmeans.labels_ == i)[0]\n",
        "    print('cluster', i)\n",
        "    print(', '.join([countries[k] for k in cluster]))"
      ],
      "metadata": {
        "id": "2VfT_cS_7Pt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9"
      ],
      "metadata": {
        "id": "x_290BldEGpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "Z = linkage(countries_vec, method='ward')\n",
        "dendrogram(Z, labels=countries)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yh-kftrhEJmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10"
      ],
      "metadata": {
        "id": "PHEQiCMnL3hj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " !pip install bhtsne"
      ],
      "metadata": {
        "id": "SFTNAnqTL6Xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import bhtsne\n",
        "\n",
        "embedded = bhtsne.tsne(np.array(countries_vec).astype(np.float64), dimensions=2, rand_seed=123)\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.scatter(np.array(embedded).T[0], np.array(embedded).T[1])\n",
        "for (x, y), name in zip(embedded, countries):#plt.annotate(name, (x, y))によって、データの名前nameを座標(x, y)の位置にアノテーションとして表示します。この処理により、各データポイントに対応する名前がプロット上に表示されます。\n",
        "    plt.annotate(name, (x, y))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xlR80bfqL7VQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pabHaI3yfM90"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}