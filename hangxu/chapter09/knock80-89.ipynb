{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyO1phq0524/FzYXhmu3RQAF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"iac11aMlbohr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688732267043,"user_tz":-540,"elapsed":25509,"user":{"displayName":"周航旭","userId":"09485546673902671804"}},"outputId":"2ac50e5f-b7ba-4e8e-d145-0ce310bf013d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["**knock80**"],"metadata":{"id":"_4W7uwWRcDdE"}},{"cell_type":"code","source":["##knock50\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","#ファイルを読み込む\n","data = pd.read_csv('drive/MyDrive/chapter09/newsCorpora.csv', sep = '\\t', header = None, names = ['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n","\n","#事例（記事）を抽出する\n","data = data.loc[data['PUBLISHER'].isin(['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']), ['TITLE', 'CATEGORY']]\n","\n","#分割する\n","##shuffle：分割する前dataをランダムにする\n","train, valid_test = train_test_split(data, test_size=0.2, shuffle=True, random_state=123, stratify=data['CATEGORY'])\n","valid, test = train_test_split(valid_test, test_size=0.5, shuffle=True, random_state=123, stratify=valid_test['CATEGORY'])\n","train.reset_index(drop=True, inplace=True)\n","valid.reset_index(drop=True, inplace=True)\n","test.reset_index(drop=True, inplace=True)"],"metadata":{"id":"avSDqu3gcHfY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import defaultdict\n","import string\n","\n","#単語の頻度集計\n","d = defaultdict(int) #辞書d\n","table = str.maketrans(string.punctuation, ' '*len(string.punctuation))  #記号をスペースに置換する\n","for text in train['TITLE']:\n","  for word in text.translate(table).split():\n","    d[word]+= 1 #単語の頻度を増やす\n","d = sorted(d.items(), key=lambda x:x[1], reverse=True) #reverse:降順\n","\n","#単語ID辞書の作成\n","word2id = {word: i+ 1 for i, (word, fre) in enumerate(d) if fre> 2}  #出現頻度が2回以上\n","\n","print(f'ID数: {len(set(word2id.values()))}\\n')\n","for key in list(word2id)[:10]:\n","    print(f'{key}: {word2id[key]}') #頻度上位10"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QO4UVO_qe5-1","executionInfo":{"status":"ok","timestamp":1688732356947,"user_tz":-540,"elapsed":313,"user":{"displayName":"周航旭","userId":"09485546673902671804"}},"outputId":"adc3bce6-1085-44b4-e22e-29a75bc0908b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ID数: 6666\n","\n","to: 1\n","s: 2\n","in: 3\n","on: 4\n","UPDATE: 5\n","as: 6\n","US: 7\n","for: 8\n","of: 9\n","The: 10\n"]}]},{"cell_type":"code","source":["#文章を入力として、その文中の単語を先頭からID化\n","def tokenizer(text, word2id=word2id, unk=0):\n","  table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n","  return [word2id.get(word, unk) for word in text.translate(table).split()] #単語を辞書word2idから対応するIDに変換\n","\n","#確認\n","text = train.iloc[1, train.columns.get_loc('TITLE')]\n","print(f'テキスト: {text}')\n","print(f'ID列: {tokenizer(text)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q2HBlNKnfuOW","executionInfo":{"status":"ok","timestamp":1688732403334,"user_tz":-540,"elapsed":7,"user":{"displayName":"周航旭","userId":"09485546673902671804"}},"outputId":"ba7467fb-8ef2-4f6c-d0a8-9573d82520e6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["テキスト: FOREX-Dollar rises on US rate speculation after Yellen comments\n","ID列: [55, 59, 161, 4, 7, 234, 3530, 26, 97, 429]\n"]}]},{"cell_type":"markdown","source":["**knock81**"],"metadata":{"id":"6TPabeCkjPaV"}},{"cell_type":"code","source":["#RNNモデルを構築\n","import torch\n","from torch import nn\n","class RNN (nn.Module):\n","  def __init__(self, vocab_size, emb_size, padding_idx, output_size, hidden_size):\n","    super().__init__()\n","    self.hidden_size = hidden_size\n","    self.emb = nn.Embedding(vocab_size, emb_size, padding_idx = padding_idx) #単語の埋め込み\n","    self.rnn = nn.RNN(emb_size, hidden_size, nonlinearity = 'tanh', batch_first = True) #tanh: 双曲線正接関数\n","    self.fc = nn.Linear(hidden_size, output_size) #隠れ層から出力層への線形変換\n","\n","  def forward(self,x): #順伝播の計算\n","    self.batch_size = x.size()[0]\n","    hidden = self.init_hidden(x.device)\n","    emb = self.emb(x) #emb.size()=(batch_size, seq_len, emb_size)\n","    out, hidden = self.rnn(emb, hidden) #out.size()=(batch_size, seq_len, hidden_size)\n","    out = self.fc(out[:, -1, :]) #最後の出力 #out.size()=(batch_size, output_size)\n","    return out\n","\n","  def init_hidden(self, device): #初期隠れ層\n","    hidden = torch.zeros(1, self.batch_size, self.hidden_size, device=device)\n","    return hidden"],"metadata":{"id":"ZMgZElkfOyMq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","\n","class CreateDataset(Dataset):\n","  def __init__(self, X, y, tokenizer):\n","    self.X = X\n","    self.y = y\n","    self.tokenizer = tokenizer\n","\n","  def __len__(self):  # len(Dataset)で返す値を指定\n","    return len(self.y)\n","\n","  def __getitem__(self, index):  # Dataset[index]で返す値を指定\n","    text = self.X[index]\n","    inputs = self.tokenizer(text)\n","\n","    return {\n","      'inputs': torch.tensor(inputs, dtype=torch.int64),\n","      'labels': torch.tensor(self.y[index], dtype=torch.int64)\n","    }\n","\n","#ラベルベクトルの作成\n","category_dict = {'b': 0, 't': 1, 'e':2, 'm':3}\n","y_train = train['CATEGORY'].map(lambda x: category_dict[x]).values\n","y_valid = valid['CATEGORY'].map(lambda x: category_dict[x]).values\n","y_test = test['CATEGORY'].map(lambda x: category_dict[x]).values\n","\n","#Datasetの作成\n","dataset_train = CreateDataset(train['TITLE'], y_train, tokenizer)\n","dataset_valid = CreateDataset(valid['TITLE'], y_valid, tokenizer)\n","dataset_test = CreateDataset(test['TITLE'], y_test, tokenizer)\n","\n","print(f'len(Dataset)の出力: {len(dataset_train)}')\n","print('Dataset[index]の出力:')\n","for var in dataset_train[1]:\n","  print(f'  {var}: {dataset_train[1][var]}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k4UB5FUbl-sQ","executionInfo":{"status":"ok","timestamp":1688732413611,"user_tz":-540,"elapsed":493,"user":{"displayName":"周航旭","userId":"09485546673902671804"}},"outputId":"993f3d55-1ef7-473b-e30e-4ea384a07176"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["len(Dataset)の出力: 10672\n","Dataset[index]の出力:\n","  inputs: tensor([  55,   59,  161,    4,    7,  234, 3530,   26,   97,  429])\n","  labels: 0\n"]}]},{"cell_type":"code","source":["#予測\n","#パラメータの設定\n","VOCAB_SIZE = len(set(word2id.values())) + 1  # 辞書のID数+paddingID\n","EMB_SIZE = 300\n","PADDING_IDX = len(set(word2id.values()))\n","OUTPUT_SIZE = 4\n","HIDDEN_SIZE = 50\n","\n","#rnnモデル\n","model = RNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, OUTPUT_SIZE, HIDDEN_SIZE)\n","\n","for i in range(10):\n"," X = dataset_train[i]['inputs']\n"," print(torch.softmax(model(X.unsqueeze(0)), dim=-1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xB_w0k29ZH6F","executionInfo":{"status":"ok","timestamp":1688732431602,"user_tz":-540,"elapsed":318,"user":{"displayName":"周航旭","userId":"09485546673902671804"}},"outputId":"b7d90217-c478-4c5a-8a6b-c623a8d87d9d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.2504, 0.3443, 0.2545, 0.1507]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.2128, 0.2875, 0.1774, 0.3223]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.1532, 0.1883, 0.1415, 0.5170]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.2132, 0.2857, 0.1356, 0.3656]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.2459, 0.2095, 0.1260, 0.4186]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.2274, 0.1552, 0.2042, 0.4132]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.2923, 0.2029, 0.2421, 0.2628]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.2383, 0.1002, 0.2612, 0.4003]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.1945, 0.3168, 0.1103, 0.3784]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.2416, 0.4049, 0.1646, 0.1889]], grad_fn=<SoftmaxBackward0>)\n"]}]},{"cell_type":"markdown","source":["**knock82**"],"metadata":{"id":"LeVxE7oYcsCh"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","from torch import optim\n","\n","##損失と正解率の計算\n","def calculate_loss_and_accuracy(model, dataset, device=None, criterion=None):\n","  dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n","  loss = 0.0\n","  total = 0\n","  correct = 0\n","  with torch.no_grad():\n","    for data in dataloader:\n","      inputs = data['inputs']\n","      labels = data['labels']\n","\n","      outputs = model(inputs) #順伝播\n","\n","      if criterion != None: #損失関数\n","        loss += criterion(outputs, labels).item()\n","\n","      pred = torch.argmax(outputs, dim=-1) #accuracy\n","      total += len(inputs)\n","      correct += (pred == labels).sum().item()\n","\n","  return loss / len(dataset), correct / total"],"metadata":{"id":"edz_5EVPkxo2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#学習を実行し、損失・正解率を返す\n","def train_model(dataset_train, dataset_valid, batch_size, model, criterion, optimizer, num_epochs, collate_fn=None, device=None):\n","  #dataloaderを作る\n","  dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n","  dataloader_valid = DataLoader(dataset_valid, batch_size=1, shuffle=False)\n","\n","  #スケジューラの設定\n","  scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs, eta_min=1e-5, last_epoch=-1)\n","\n","  #学習\n","  for epoch in range(num_epochs):\n","\n","    model.train() #訓練モードに設定\n","    for data in dataloader_train:\n","      optimizer.zero_grad() #勾配をゼロで初期化\n","\n","      #順伝播+誤差逆伝播+重み更新\n","      inputs = data['inputs']\n","      labels = data['labels']\n","      outputs = model(inputs)\n","      loss = criterion(outputs, labels)\n","      loss.backward()\n","      optimizer.step()\n","\n","    model.eval() #評価モードに設定\n","\n","    #損失と正解率\n","    loss_train, acc_train = calculate_loss_and_accuracy(model, dataset_train, device, criterion=criterion)\n","    loss_valid, acc_valid = calculate_loss_and_accuracy(model, dataset_valid, device, criterion=criterion)\n","\n","    #チェックポイントの保存\n","    torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, f'checkpoint{epoch + 1}.pt')\n","    print(f'epoch: {epoch + 1}, loss_train: {loss_train:.4f}, accuracy_train: {acc_train:.4f}, loss_valid: {loss_valid:.4f}, accuracy_valid: {acc_valid:.4f}')\n","\n","    if epoch > 10: #stop\n","      break\n","\n","    scheduler.step() #スケジューラを1step進める\n"],"metadata":{"id":"rDlWhGanBQvp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#parameters\n","VOCAB_SIZE = len(set(word2id.values())) + 1\n","EMB_SIZE = 300\n","PADDING_IDX = len(set(word2id.values()))\n","OUTPUT_SIZE = 4\n","HIDDEN_SIZE = 50\n","LEARNING_RATE = 1e-3\n","BATCH_SIZE = 1\n","NUM_EPOCHS = 10\n","\n","model = RNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, OUTPUT_SIZE, HIDDEN_SIZE) #model\n","criterion = nn.CrossEntropyLoss() #損失関数\n","optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n","\n","#モデルの学習\n","train_model(dataset_train, dataset_valid, BATCH_SIZE, model, criterion, optimizer, NUM_EPOCHS)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7cHraKSPBXZU","executionInfo":{"status":"ok","timestamp":1688733260712,"user_tz":-540,"elapsed":374768,"user":{"displayName":"周航旭","userId":"09485546673902671804"}},"outputId":"bcf1e1c6-9cb8-40fa-ec79-edf0a9bdac1a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch: 1, loss_train: 1.1079, accuracy_train: 0.5134, loss_valid: 1.1316, accuracy_valid: 0.4955\n","epoch: 2, loss_train: 1.0210, accuracy_train: 0.5772, loss_valid: 1.0735, accuracy_valid: 0.5397\n","epoch: 3, loss_train: 0.8852, accuracy_train: 0.6637, loss_valid: 0.9698, accuracy_valid: 0.6327\n","epoch: 4, loss_train: 0.7280, accuracy_train: 0.7429, loss_valid: 0.8479, accuracy_valid: 0.6964\n","epoch: 5, loss_train: 0.6422, accuracy_train: 0.7724, loss_valid: 0.7880, accuracy_valid: 0.7264\n","epoch: 6, loss_train: 0.5729, accuracy_train: 0.7943, loss_valid: 0.7443, accuracy_valid: 0.7376\n","epoch: 7, loss_train: 0.5203, accuracy_train: 0.8107, loss_valid: 0.7081, accuracy_valid: 0.7549\n","epoch: 8, loss_train: 0.4900, accuracy_train: 0.8216, loss_valid: 0.6922, accuracy_valid: 0.7511\n","epoch: 9, loss_train: 0.4744, accuracy_train: 0.8269, loss_valid: 0.6934, accuracy_valid: 0.7474\n","epoch: 10, loss_train: 0.4703, accuracy_train: 0.8287, loss_valid: 0.6946, accuracy_valid: 0.7496\n"]}]},{"cell_type":"markdown","source":["**knock83**"],"metadata":{"id":"ySntKUHe13C3"}},{"cell_type":"code","source":["##系列の長さに基づいて自動的にパディングが行われます\n","class Padsequence():\n","  def __init__(self, padding_idx):\n","    self.padding_idx = padding_idx\n","\n","  def __call__(self, batch): #ミニバッチを取り出す\n","    sorted_batch = sorted(batch, key=lambda x: x['inputs'].shape[0], reverse=True)\n","    sequences = [x['inputs'] for x in sorted_batch]\n","    sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=self.padding_idx)\n","    labels = torch.LongTensor([x['labels'] for x in sorted_batch])\n","\n","    return {'inputs': sequences_padded, 'labels': labels}"],"metadata":{"id":"kdOj1_P614Zo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["##損失と正解率の計算\n","def calculate_loss_and_accuracy(model, dataset, device=None, criterion=None):\n","  dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n","  loss = 0.0\n","  total = 0\n","  correct = 0\n","  with torch.no_grad():\n","    for data in dataloader:\n","      inputs = data['inputs'].to(device)\n","      labels = data['labels'].to(device)\n","\n","      outputs = model(inputs) #順伝播\n","\n","      if criterion != None: #損失関数\n","        loss += criterion(outputs, labels).item()\n","\n","      pred = torch.argmax(outputs, dim=-1) #accuracy\n","      total += len(inputs)\n","      correct += (pred == labels).sum().item()\n","\n","  return loss / len(dataset), correct / total"],"metadata":{"id":"incNIY9x8ugc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#学習を実行し、損失・正解率を返す GPUを使う\n","def train_model(dataset_train, dataset_valid, batch_size, model, criterion, optimizer, num_epochs, collate_fn=None, device=None):\n","  model.to(device)\n","\n","  #dataloaderを作る\n","  dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n","  dataloader_valid = DataLoader(dataset_valid, batch_size=1, shuffle=False)\n","\n","  #スケジューラの設定\n","  scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs, eta_min=1e-5, last_epoch=-1)\n","\n","  #学習\n","  for epoch in range(num_epochs):\n","    model.train() #訓練モードに設定\n","    for data in dataloader_train:\n","      optimizer.zero_grad() #勾配をゼロで初期化\n","\n","      #順伝播+誤差逆伝播+重み更新\n","      inputs = data['inputs'].to(device)\n","      labels = data['labels'].to(device)\n","      outputs = model(inputs)\n","      loss = criterion(outputs, labels)\n","      loss.backward()\n","      optimizer.step()\n","\n","    model.eval() #評価モードに設定\n","\n","    #損失と正解率\n","    loss_train, acc_train = calculate_loss_and_accuracy(model, dataset_train, device, criterion=criterion)\n","    loss_valid, acc_valid = calculate_loss_and_accuracy(model, dataset_valid, device, criterion=criterion)\n","\n","    #チェックポイントの保存\n","    torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, f'checkpoint{epoch + 1}.pt')\n","    print(f'epoch: {epoch + 1}, loss_train: {loss_train:.4f}, accuracy_train: {acc_train:.4f}, loss_valid: {loss_valid:.4f}, accuracy_valid: {acc_valid:.4f}')\n","\n","    if epoch > 10: #stop\n","      break\n","\n","    scheduler.step() #スケジューラを1step進める\n"],"metadata":{"id":"XiDmrGir7yZn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#parameters\n","VOCAB_SIZE = len(set(word2id.values())) + 1\n","EMB_SIZE = 300\n","PADDING_IDX = len(set(word2id.values()))\n","OUTPUT_SIZE = 4\n","HIDDEN_SIZE = 50\n","LEARNING_RATE = 5e-2\n","BATCH_SIZE = 32\n","NUM_EPOCHS = 10\n","\n","model = RNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, OUTPUT_SIZE, HIDDEN_SIZE) #model\n","criterion = nn.CrossEntropyLoss() #損失関数\n","optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n","device = torch.device('cuda')\n","\n","#モデルの学習\n","train_model(dataset_train, dataset_valid, BATCH_SIZE, model, criterion, optimizer, NUM_EPOCHS, collate_fn=Padsequence(PADDING_IDX), device=device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cNEsEUiL6_xQ","executionInfo":{"status":"ok","timestamp":1688733958755,"user_tz":-540,"elapsed":108247,"user":{"displayName":"周航旭","userId":"09485546673902671804"}},"outputId":"75d8544a-65af-49ad-d253-1c90518b949a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch: 1, loss_train: 1.2721, accuracy_train: 0.3936, loss_valid: 1.2799, accuracy_valid: 0.4003\n","epoch: 2, loss_train: 1.1861, accuracy_train: 0.4897, loss_valid: 1.1998, accuracy_valid: 0.4775\n","epoch: 3, loss_train: 1.1241, accuracy_train: 0.5683, loss_valid: 1.1848, accuracy_valid: 0.5427\n","epoch: 4, loss_train: 1.1464, accuracy_train: 0.5397, loss_valid: 1.2052, accuracy_valid: 0.5187\n","epoch: 5, loss_train: 1.1896, accuracy_train: 0.4621, loss_valid: 1.2018, accuracy_valid: 0.4625\n","epoch: 6, loss_train: 1.1761, accuracy_train: 0.4925, loss_valid: 1.2019, accuracy_valid: 0.4820\n","epoch: 7, loss_train: 1.1471, accuracy_train: 0.5112, loss_valid: 1.1795, accuracy_valid: 0.4993\n","epoch: 8, loss_train: 1.2044, accuracy_train: 0.4640, loss_valid: 1.2183, accuracy_valid: 0.4595\n","epoch: 9, loss_train: 1.2245, accuracy_train: 0.4603, loss_valid: 1.2389, accuracy_valid: 0.4513\n","epoch: 10, loss_train: 1.2272, accuracy_train: 0.4608, loss_valid: 1.2421, accuracy_valid: 0.4505\n"]}]},{"cell_type":"markdown","source":["**knock84**"],"metadata":{"id":"AIIGTUrW8iwK"}},{"cell_type":"code","source":["import numpy as np\n","from gensim.models import KeyedVectors\n","\n","#学習済みモデルのロード\n","model = KeyedVectors.load_word2vec_format('drive/MyDrive/chapter09/GoogleNews-vectors-negative300.bin.gz', binary=True)\n","\n","#学習済み単語ベクトルの取得\n","VOCAB_SIZE = len(set(word2id.values())) + 1\n","EMB_SIZE = 300 #単語ベクトルの次元数\n","weights = np.zeros((VOCAB_SIZE, EMB_SIZE)) #学習済み単語ベクトルを格納する\n","words_in_pretrained = 0\n","for i, word in enumerate(word2id.keys()):\n","  try:\n","    weights[i] = model[word]\n","    words_in_pretrained += 1 #学習済み単語ベクトルの単語の数\n","  except KeyError:\n","    weights[i] = np.random.normal(scale=0.4, size=(EMB_SIZE,)) #ランダムな値で行を初期化する\n","weights = torch.from_numpy(weights.astype((np.float32))) #numpy配列からtorch.Tensorに変換\n","\n","print(f'学習済みベクトル利用単語数: {words_in_pretrained} / {VOCAB_SIZE}')\n","print(weights.size())"],"metadata":{"id":"Dvbn9Zfo-NO2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688734050772,"user_tz":-540,"elapsed":71532,"user":{"displayName":"周航旭","userId":"09485546673902671804"}},"outputId":"70037aa0-badb-4992-c641-47b784908a04"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["学習済みベクトル利用単語数: 6514 / 6667\n","torch.Size([6667, 300])\n"]}]},{"cell_type":"code","source":["class RNN(nn.Module):\n","  def __init__(self, vocab_size, emb_size, padding_idx, output_size, hidden_size, num_layers, emb_weights=None, bidirectional=False):\n","    super().__init__()\n","    self.hidden_size = hidden_size #隠れ状態の次元数\n","    self.num_layers = num_layers\n","    self.num_directions = bidirectional + 1  #単方向：1、双方向：2\n","    if emb_weights != None:  #emb_weights: 埋め込み層の重み\n","      self.emb = nn.Embedding.from_pretrained(emb_weights, padding_idx=padding_idx)\n","    else:\n","      self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n","    self.rnn = nn.RNN(emb_size, hidden_size, num_layers, nonlinearity='tanh', bidirectional=bidirectional, batch_first=True)\n","    self.fc = nn.Linear(hidden_size * self.num_directions, output_size) #線形\n","\n","  def forward(self, x): #順方向の計算が行う\n","    self.batch_size = x.size()[0]\n","    hidden = self.init_hidden(x.device)  #h0のゼロベクトルを作成\n","    emb = self.emb(x)\n","    #emb.size() = (batch_size, seq_len, emb_size)\n","    out, hidden = self.rnn(emb, hidden)\n","    #out.size() = (batch_size, seq_len, hidden_size * num_directions)\n","    out = self.fc(out[:, -1, :])\n","    #out.size() = (batch_size, output_size)\n","    return out\n","\n","  def init_hidden(self, device): #RNNの初期隠れ状態をゼロベクトルで初期化\n","    hidden = torch.zeros(self.num_layers * self.num_directions, self.batch_size, self.hidden_size, device=device)\n","    return hidden"],"metadata":{"id":"ymZnniZH5Q88"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# パラメータの設定\n","VOCAB_SIZE = len(set(word2id.values())) + 1\n","EMB_SIZE = 300\n","PADDING_IDX = len(set(word2id.values()))\n","OUTPUT_SIZE = 4\n","HIDDEN_SIZE = 50\n","NUM_LAYERS = 1\n","LEARNING_RATE = 5e-2\n","BATCH_SIZE = 32\n","NUM_EPOCHS = 10\n","\n","model = RNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, OUTPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, emb_weights=weights)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n","device = torch.device('cuda')\n","\n","# モデルの学習\n","train_model(dataset_train, dataset_valid, BATCH_SIZE, model, criterion, optimizer, NUM_EPOCHS, collate_fn=Padsequence(PADDING_IDX), device=device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R86QaHqlHHUh","executionInfo":{"status":"ok","timestamp":1688734247626,"user_tz":-540,"elapsed":95573,"user":{"displayName":"周航旭","userId":"09485546673902671804"}},"outputId":"a16fd54e-0265-4454-a794-d2757e279fc5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch: 1, loss_train: 1.1695, accuracy_train: 0.4620, loss_valid: 1.1834, accuracy_valid: 0.4708\n","epoch: 2, loss_train: 1.1678, accuracy_train: 0.4447, loss_valid: 1.1614, accuracy_valid: 0.4460\n","epoch: 3, loss_train: 1.1000, accuracy_train: 0.5742, loss_valid: 1.1463, accuracy_valid: 0.5412\n","epoch: 4, loss_train: 1.0367, accuracy_train: 0.6195, loss_valid: 1.0948, accuracy_valid: 0.5787\n","epoch: 5, loss_train: 1.0422, accuracy_train: 0.6129, loss_valid: 1.1120, accuracy_valid: 0.5667\n","epoch: 6, loss_train: 1.0449, accuracy_train: 0.6012, loss_valid: 1.1170, accuracy_valid: 0.5600\n","epoch: 7, loss_train: 0.9810, accuracy_train: 0.6391, loss_valid: 1.0552, accuracy_valid: 0.5967\n","epoch: 8, loss_train: 0.9410, accuracy_train: 0.6585, loss_valid: 1.0109, accuracy_valid: 0.6229\n","epoch: 9, loss_train: 0.9351, accuracy_train: 0.6594, loss_valid: 1.0050, accuracy_valid: 0.6222\n","epoch: 10, loss_train: 0.9392, accuracy_train: 0.6575, loss_valid: 1.0101, accuracy_valid: 0.6192\n"]}]},{"cell_type":"markdown","source":["**knock85**"],"metadata":{"id":"W3UrBX-JHceF"}},{"cell_type":"code","source":["#parameters\n","VOCAB_SIZE = len(set(word2id.values())) + 1\n","EMB_SIZE = 300\n","PADDING_IDX = len(set(word2id.values()))\n","OUTPUT_SIZE = 4\n","HIDDEN_SIZE = 50\n","NUM_LAYERS = 2\n","LEARNING_RATE = 5e-2\n","BATCH_SIZE = 32\n","NUM_EPOCHS = 10\n","\n","model = RNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, OUTPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, emb_weights=weights, bidirectional=True)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n","device = torch.device('cuda')\n","\n","#モデルの学習\n","train_model(dataset_train, dataset_valid, BATCH_SIZE, model, criterion, optimizer, NUM_EPOCHS, collate_fn=Padsequence(PADDING_IDX), device=device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G1Bh31MUOcxc","executionInfo":{"status":"ok","timestamp":1688734392552,"user_tz":-540,"elapsed":107069,"user":{"displayName":"周航旭","userId":"09485546673902671804"}},"outputId":"91f471f5-53ca-4e7c-a781-ed48c071212b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch: 1, loss_train: 1.1713, accuracy_train: 0.4545, loss_valid: 1.1871, accuracy_valid: 0.4513\n","epoch: 2, loss_train: 1.1170, accuracy_train: 0.5411, loss_valid: 1.1579, accuracy_valid: 0.5052\n","epoch: 3, loss_train: 1.0565, accuracy_train: 0.5986, loss_valid: 1.1092, accuracy_valid: 0.5562\n","epoch: 4, loss_train: 1.0738, accuracy_train: 0.5926, loss_valid: 1.1356, accuracy_valid: 0.5502\n","epoch: 5, loss_train: 1.0512, accuracy_train: 0.6048, loss_valid: 1.1116, accuracy_valid: 0.5637\n","epoch: 6, loss_train: 0.9888, accuracy_train: 0.6377, loss_valid: 1.0409, accuracy_valid: 0.6049\n","epoch: 7, loss_train: 0.9656, accuracy_train: 0.6466, loss_valid: 1.0182, accuracy_valid: 0.6214\n","epoch: 8, loss_train: 0.9566, accuracy_train: 0.6483, loss_valid: 1.0159, accuracy_valid: 0.6177\n","epoch: 9, loss_train: 0.9585, accuracy_train: 0.6470, loss_valid: 1.0256, accuracy_valid: 0.6147\n","epoch: 10, loss_train: 0.9631, accuracy_train: 0.6448, loss_valid: 1.0331, accuracy_valid: 0.6132\n"]}]},{"cell_type":"markdown","source":["**knock.86**"],"metadata":{"id":"qJ5jdEcHXQcb"}},{"cell_type":"code","source":["#テキスト分類モデル\n","from torch.nn import functional as F\n","\n","class CNN(nn.Module):\n","  def __init__(self, vocab_size, emb_size, padding_idx, output_size, out_channels, kernel_heights, stride, padding, emb_weights=None):\n","    super().__init__()\n","    if emb_weights != None:  #指定があれば埋め込み層の重みをemb_weightsで初期化\n","      self.emb = nn.Embedding.from_pretrained(emb_weights, padding_idx=padding_idx) #入力単語の埋め込み\n","    else:\n","      self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n","    self.conv = nn.Conv2d(1, out_channels, (kernel_heights, emb_size), stride, (padding, 0)) #1次元の畳み込み、入力のチャネル数：1\n","    self.drop = nn.Dropout(0.3) #過学習を防ぐ\n","    self.fc = nn.Linear(out_channels, output_size) #線形変換\n","\n","  def forward(self, x): #モデルの順伝播\n","    #x.size() = (batch_size, seq_len)\n","    emb = self.emb(x).unsqueeze(1)\n","    #emb.size() = (batch_size, 1, seq_len, emb_size)\n","    conv = self.conv(emb) #埋め込みベクトルを畳み込み層に通し、畳み込み演算\n","    #conv.size() = (batch_size, out_channels, seq_len, 1)\n","    act = F.relu(conv.squeeze(3)) #3番目の次元を1次元に圧縮し、ReLU関数を適用\n","    #act.size() = (batch_size, out_channels, seq_len)\n","    max_pool = F.max_pool1d(act, act.size()[2]) # 1次元の最大プーリング\n","    #max_pool.size() = (batch_size, out_channels, 1) -> seq_len方向に最大値を取得\n","    out = self.fc(self.drop(max_pool.squeeze(2)))\n","    #out.size() = (batch_size, output_size)\n","    return out"],"metadata":{"id":"gFDo6qjgXN5j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#パラメータの設定\n","VOCAB_SIZE = len(set(word2id.values())) + 1 #単語の種類数\n","EMB_SIZE = 300\n","PADDING_IDX = len(set(word2id.values()))\n","OUTPUT_SIZE = 4\n","OUT_CHANNELS = 100\n","KERNEL_HEIGHTS = 3 #畳み込みカーネルの高さ\n","STRIDE = 1\n","PADDING = 1\n","\n","#モデルの定義\n","model = CNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, OUTPUT_SIZE, OUT_CHANNELS, KERNEL_HEIGHTS, STRIDE, PADDING, emb_weights=weights)\n","\n","#先頭10件の予測値取得\n","for i in range(10):\n","  X = dataset_train[i]['inputs']\n","  print(torch.softmax(model(X.unsqueeze(0)), dim=-1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pgUOelZcXXtt","executionInfo":{"status":"ok","timestamp":1688734424042,"user_tz":-540,"elapsed":497,"user":{"displayName":"周航旭","userId":"09485546673902671804"}},"outputId":"e463fda4-2b2e-4abc-8562-a5b1c801d140"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.2258, 0.2948, 0.2645, 0.2149]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.2236, 0.2629, 0.2687, 0.2448]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.2170, 0.2726, 0.2828, 0.2276]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.2178, 0.3150, 0.2420, 0.2252]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.2385, 0.2681, 0.2478, 0.2456]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.2145, 0.3273, 0.2680, 0.1902]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.2311, 0.2439, 0.3267, 0.1983]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.2513, 0.2635, 0.2694, 0.2159]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.2373, 0.2636, 0.2647, 0.2344]], grad_fn=<SoftmaxBackward0>)\n","tensor([[0.1971, 0.2009, 0.3086, 0.2935]], grad_fn=<SoftmaxBackward0>)\n"]}]},{"cell_type":"markdown","source":["**knock87**"],"metadata":{"id":"4XaVthYUmKnd"}},{"cell_type":"code","source":["#パラメータの設定\n","VOCAB_SIZE = len(set(word2id.values())) + 1\n","EMB_SIZE = 300\n","PADDING_IDX = len(set(word2id.values()))\n","OUTPUT_SIZE = 4\n","OUT_CHANNELS = 100\n","KERNEL_HEIGHTS = 3\n","STRIDE = 1\n","PADDING = 1\n","LEARNING_RATE = 5e-2\n","BATCH_SIZE = 64\n","NUM_EPOCHS = 10\n","\n","#モデルの定義\n","model = CNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, OUTPUT_SIZE, OUT_CHANNELS, KERNEL_HEIGHTS, STRIDE, PADDING, emb_weights=weights)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n","device = torch.device('cuda')\n","\n","#モデルの学習\n","log = train_model(dataset_train, dataset_valid, BATCH_SIZE, model, criterion, optimizer, NUM_EPOCHS, collate_fn=Padsequence(PADDING_IDX), device=device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5A01n89NmOKm","executionInfo":{"status":"ok","timestamp":1688734518411,"user_tz":-540,"elapsed":91073,"user":{"displayName":"周航旭","userId":"09485546673902671804"}},"outputId":"d4117f2b-96c5-4af8-ca9f-6752afa83847"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch: 1, loss_train: 1.0855, accuracy_train: 0.5352, loss_valid: 1.0886, accuracy_valid: 0.5420\n","epoch: 2, loss_train: 1.0082, accuracy_train: 0.6297, loss_valid: 1.0237, accuracy_valid: 0.6079\n","epoch: 3, loss_train: 0.9292, accuracy_train: 0.6719, loss_valid: 0.9616, accuracy_valid: 0.6484\n","epoch: 4, loss_train: 0.8668, accuracy_train: 0.6951, loss_valid: 0.9146, accuracy_valid: 0.6717\n","epoch: 5, loss_train: 0.8222, accuracy_train: 0.7127, loss_valid: 0.8802, accuracy_valid: 0.6852\n","epoch: 6, loss_train: 0.7891, accuracy_train: 0.7237, loss_valid: 0.8601, accuracy_valid: 0.6934\n","epoch: 7, loss_train: 0.7674, accuracy_train: 0.7319, loss_valid: 0.8448, accuracy_valid: 0.7039\n","epoch: 8, loss_train: 0.7541, accuracy_train: 0.7351, loss_valid: 0.8367, accuracy_valid: 0.7016\n","epoch: 9, loss_train: 0.7479, accuracy_train: 0.7369, loss_valid: 0.8326, accuracy_valid: 0.7024\n","epoch: 10, loss_train: 0.7463, accuracy_train: 0.7369, loss_valid: 0.8316, accuracy_valid: 0.7039\n"]}]},{"cell_type":"markdown","source":["**knock88**"],"metadata":{"id":"d665c9okmioW"}},{"cell_type":"code","source":["#テキスト分類モデル　複数の畳み込み層\n","from torch.nn import functional as F\n","\n","class textCNN(nn.Module):\n","  def __init__(self, vocab_size, emb_size, padding_idx, output_size, out_channels, conv_params, drop_rate, emb_weights=None):\n","    super().__init__()\n","    if emb_weights != None:  #指定があれば埋め込み層の重みをemb_weightsで初期化\n","      self.emb = nn.Embedding.from_pretrained(emb_weights, padding_idx=padding_idx) #入力単語の埋め込み\n","    else:\n","      self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n","    self.convs = nn.ModuleList([nn.Conv2d(1, out_channels, (kernel_height, emb_size), padding=(padding, 0)) for kernel_height, padding in conv_params])\n","    self.drop = nn.Dropout(drop_rate) #過学習を防ぐ\n","    self.fc = nn.Linear(len(conv_params) * out_channels, output_size) #線形変換\n","\n","  def forward(self, x): #モデルの順伝播\n","    emb = self.emb(x).unsqueeze(1)\n","    conv = [F.relu(conv(emb)).squeeze(3) for i, conv in enumerate(self.convs)] #埋め込みベクトルを畳み込み層に通し、畳み込み演算\n","    max_pool = [F.max_pool1d(i, i.size(2)) for i in conv] # 1次元の最大プーリング\n","    max_pool_cat = torch.cat(max_pool, 1)\n","    out = self.fc(self.drop(max_pool_cat.squeeze(2)))\n","    return out"],"metadata":{"id":"THhdiqctmmTg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install optuna"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xMCkC4tDsDqE","executionInfo":{"status":"ok","timestamp":1688734547265,"user_tz":-540,"elapsed":5793,"user":{"displayName":"周航旭","userId":"09485546673902671804"}},"outputId":"907f9c16-32f1-4deb-eb55-b63df77c2e44"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting optuna\n","  Downloading optuna-3.2.0-py3-none-any.whl (390 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/390.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.6/390.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n","  Downloading alembic-1.11.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting cmaes>=0.9.1 (from optuna)\n","  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n","Collecting colorlog (from optuna)\n","  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.1)\n","Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.16)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.65.0)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0)\n","Collecting Mako (from alembic>=1.5.0->optuna)\n","  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.6.3)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n","Installing collected packages: Mako, colorlog, cmaes, alembic, optuna\n","Successfully installed Mako-1.2.4 alembic-1.11.1 cmaes-0.9.1 colorlog-6.7.0 optuna-3.2.0\n"]}]},{"cell_type":"code","source":["import optuna\n","\n","def objective(trial):\n","  #チューニング対象パラメータのセット\n","  emb_size = int(trial.suggest_discrete_uniform('emb_size', 100, 400, 100))\n","  out_channels = int(trial.suggest_discrete_uniform('out_channels', 50, 200, 50))\n","  drop_rate = trial.suggest_discrete_uniform('drop_rate', 0.0, 0.5, 0.1)\n","  learning_rate = trial.suggest_loguniform('learning_rate', 5e-4, 5e-2)\n","  momentum = trial.suggest_discrete_uniform('momentum', 0.5, 0.9, 0.1)\n","  batch_size = int(trial.suggest_discrete_uniform('batch_size', 16, 128, 16))\n","\n","  #固定パラメータの設定\n","  VOCAB_SIZE = len(set(word2id.values())) + 1\n","  PADDING_IDX = len(set(word2id.values()))\n","  OUTPUT_SIZE = 4\n","  CONV_PARAMS = [[2, 0], [3, 1], [4, 2]]\n","  NUM_EPOCHS = 30\n","\n","  # モデルの定義\n","  model = textCNN(VOCAB_SIZE, emb_size, PADDING_IDX, OUTPUT_SIZE, out_channels, CONV_PARAMS, drop_rate, emb_weights=weights)\n","\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n","  device = torch.cuda.set_device(0)\n","\n","  log = train_model(dataset_train, dataset_valid, batch_size, model, criterion, optimizer, NUM_EPOCHS, collate_fn=Padsequence(PADDING_IDX), device=device)\n","  loss_valid, _ = calculate_loss_and_accuracy(model, dataset_valid, device, criterion=criterion)\n","\n","  return loss_valid"],"metadata":{"id":"DLbJcacJrTEs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#最適化\n","study = optuna.create_study()\n","study.optimize(objective, timeout=7200)\n","\n","#結果の表示\n","print('Best trial:')\n","trial = study.best_trial\n","print('  Value: {:.3f}'.format(trial.value))\n","print('  Params: ')\n","for key, value in trial.params.items():\n","  print('    {}: {}'.format(key, value))"],"metadata":{"id":"-BeWMH8LbaGh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# パラメータの設定\n","VOCAB_SIZE = len(set(word2id.values())) + 1\n","EMB_SIZE = int(trial.params['emb_size'])\n","PADDING_IDX = len(set(word2id.values()))\n","OUTPUT_SIZE = 4\n","OUT_CHANNELS = int(trial.params['out_channels'])\n","CONV_PARAMS = [[2, 0], [3, 1], [4, 2]]\n","DROP_RATE = trial.params['drop_rate']\n","LEARNING_RATE = trial.params['learning_rate']\n","BATCH_SIZE = int(trial.params['batch_size'])\n","NUM_EPOCHS = 30\n","\n","#モデルの定義\n","model = textCNN(VOCAB_SIZE, EMB_SIZE, PADDING_IDX, OUTPUT_SIZE, OUT_CHANNELS, CONV_PARAMS, DROP_RATE, emb_weights=weights)\n","print(model)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n","device = torch.cuda.set_device(0)\n","\n","\n","log = train_model(dataset_train, dataset_valid, BATCH_SIZE, model, criterion, optimizer, NUM_EPOCHS, collate_fn=Padsequence(PADDING_IDX), device=device)"],"metadata":{"id":"p8X3J2KkbeDU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**knock89**"],"metadata":{"id":"wquiZ9ILvr05"}},{"cell_type":"code","source":["#..."],"metadata":{"id":"SJ7wZBBYvuKC"},"execution_count":null,"outputs":[]}]}